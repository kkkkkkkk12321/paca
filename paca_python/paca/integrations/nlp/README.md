# ğŸ‡°ğŸ‡· Korean NLP Integration System

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”

í•œêµ­ì–´ ìì—°ì–´ì²˜ë¦¬ë¥¼ ìœ„í•œ í†µí•© ì‹œìŠ¤í…œìœ¼ë¡œ, KoNLPy ê¸°ë°˜ì˜ í˜•íƒœì†Œ ë¶„ì„, êµ¬ë¬¸ ë¶„ì„, ì˜ë¯¸ ë¶„ì„, ê·¸ë¦¬ê³  í•œêµ­ ë¬¸í™”ì  ë§¥ë½ ì²˜ë¦¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤. PACAì˜ Phase 2.2 êµ¬í˜„ìœ¼ë¡œ í•œêµ­ì–´ íŠ¹í™” AI ê¸°ëŠ¥ì„ ì™„ì„±í•©ë‹ˆë‹¤.

## ğŸ“ í´ë”/íŒŒì¼ êµ¬ì¡°

```
integrations/nlp/
â”œâ”€â”€ __init__.py                    # í†µí•© NLP ì‹œìŠ¤í…œ ì¸í„°í˜ì´ìŠ¤ ë° KoreanNLPSystem í´ë˜ìŠ¤
â”œâ”€â”€ README.md                      # ë³¸ ë¬¸ì„œ (9ê°œ ì„¹ì…˜ ì™„ì „ ë¬¸ì„œí™”)
â”œâ”€â”€ konlpy_integration.py          # KoNLPy ë¼ì´ë¸ŒëŸ¬ë¦¬ í†µí•© ë° ë‹¤ì¤‘ ë¶„ì„ê¸° ì§€ì›
â”œâ”€â”€ korean_tokenizer.py            # í•œêµ­ì–´ í† í¬ë‚˜ì´ì € (ì¡´ëŒ“ë§ ìˆ˜ì¤€ ê°ì§€ í¬í•¨)
â”œâ”€â”€ morphology_analyzer.py         # í˜•íƒœì†Œ ë¶„ì„ê¸° (ë¶ˆê·œì¹™ í™œìš©, ë³µí•©ì–´ ë¶„ì„)
â”œâ”€â”€ syntax_parser.py               # êµ¬ë¬¸ ë¶„ì„ê¸° (ì˜ì¡´ êµ¬ì¡° ë¶„ì„, êµ¬ë¬¸ ì—­í• )
â”œâ”€â”€ semantic_analyzer.py           # ì˜ë¯¸ ë¶„ì„ê¸° (ê°ì • ë¶„ì„, ê°œì²´ëª… ì¸ì‹)
â””â”€â”€ cultural_context.py            # ë¬¸í™”ì  ë§¥ë½ ì²˜ë¦¬ê¸° (ì¡´ëŒ“ë§, ì‚¬íšŒì  ê´€ê³„)
```

### íŒŒì¼ë³„ ìƒì„¸ ê¸°ëŠ¥

- **`__init__.py`**: í†µí•© NLP ì‹œìŠ¤í…œ (`KoreanNLPSystem`) ë° ê°„í¸ ì‚¬ìš© í•¨ìˆ˜ë“¤ ì œê³µ
- **`konlpy_integration.py`**: Mecab, Okt, Komoran ë“± ë‹¤ì¤‘ ë¶„ì„ê¸° ì§€ì› ë° ì„±ëŠ¥ ìµœì í™”
- **`korean_tokenizer.py`**: ì¡´ëŒ“ë§ ìˆ˜ì¤€ë³„ í† í°í™”, íŠ¹ìˆ˜ í† í° ì²˜ë¦¬, í•œêµ­ì–´ ë¬¸ì íŒ¨í„´ ì¸ì‹
- **`morphology_analyzer.py`**: ë¶ˆê·œì¹™ ë™ì‚¬ ì²˜ë¦¬, ë³µí•©ì–´ ë¶„ì„, ìŒì„±í•™ì  ë³€í™” ë¶„ì„
- **`syntax_parser.py`**: ì˜ì¡´ êµ¬ë¬¸ ë¶„ì„, êµ¬ë¬¸ ì—­í•  í• ë‹¹, êµ¬ êµ¬ì¡° ì¸ì‹
- **`semantic_analyzer.py`**: ê°ì • ê·¹ì„± ë¶„ì„, ê°ì • ìœ í˜• ë¶„ë¥˜, ê°œì²´ëª… ì¶”ì¶œ, ê´€ê³„ ì¶”ì¶œ
- **`cultural_context.py`**: ì¡´ëŒ“ë§ ì²´ê³„ ë¶„ì„, ì‚¬íšŒì  ê´€ê³„ ì¶”ë¡ , ë¬¸í™”ì  ê°œë… ì¸ì‹

## âš™ï¸ ê¸°ëŠ¥ ìš”êµ¬ì‚¬í•­

### ì…ë ¥/ì¶œë ¥ ì¸í„°í˜ì´ìŠ¤
- **ì…ë ¥**: í•œêµ­ì–´ í…ìŠ¤íŠ¸ (ë¬¸ì¥, ë‹¨ë½, ë˜ëŠ” ê¸´ í…ìŠ¤íŠ¸)
- **ì¶œë ¥**: JSON í˜•íƒœì˜ êµ¬ì¡°í™”ëœ ë¶„ì„ ê²°ê³¼

### í•µì‹¬ ë¡œì§ íë¦„
```mermaid
graph TD
    A[í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì…ë ¥] --> B[í† í¬ë‚˜ì´ì €]
    B --> C[í˜•íƒœì†Œ ë¶„ì„ê¸°]
    C --> D[êµ¬ë¬¸ ë¶„ì„ê¸°]
    D --> E[ì˜ë¯¸ ë¶„ì„ê¸°]
    E --> F[ë¬¸í™”ì  ë§¥ë½ ì²˜ë¦¬ê¸°]
    F --> G[í†µí•© ê²°ê³¼ ë°˜í™˜]

    B --> H[ì¡´ëŒ“ë§ ìˆ˜ì¤€ ê°ì§€]
    C --> I[ë¶ˆê·œì¹™ í™œìš© ì²˜ë¦¬]
    D --> J[êµ¬ë¬¸ ì—­í•  í• ë‹¹]
    E --> K[ê°ì •/ê°ì„± ë¶„ì„]
    F --> L[ì‚¬íšŒì  ê´€ê³„ ì¶”ë¡ ]
```

### ë¶„ì„ ê¸°ëŠ¥
1. **í† í°í™”**: í˜•íƒœì†Œ ë‹¨ìœ„ ë¶„ë¦¬, ì¡´ëŒ“ë§ ìˆ˜ì¤€ë³„ ë¶„ë¥˜, íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬
2. **í˜•íƒœì†Œ ë¶„ì„**: POS íƒœê¹…, ë¶ˆê·œì¹™ í™œìš© ì •ê·œí™”, ë³µí•©ì–´ ë¶„í•´
3. **êµ¬ë¬¸ ë¶„ì„**: ì˜ì¡´ êµ¬ì¡° ë¶„ì„, êµ¬ë¬¸ ì—­í•  í• ë‹¹ (ì£¼ì–´, ëª©ì ì–´, ì„œìˆ ì–´)
4. **ì˜ë¯¸ ë¶„ì„**: ê°ì • ê·¹ì„± (ê¸ì •/ë¶€ì •/ì¤‘ë¦½), ê°œì²´ëª… ì¸ì‹, ê´€ê³„ ì¶”ì¶œ
5. **ë¬¸í™” ë¶„ì„**: ì¡´ëŒ“ë§ ì ì ˆì„±, ì‚¬íšŒì  ê±°ë¦¬ê°, í•œêµ­ ë¬¸í™” ê°œë… ì¸ì‹

## ğŸ› ï¸ ê¸°ìˆ ì  ìš”êµ¬ì‚¬í•­

### í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ë° í”„ë ˆì„ì›Œí¬
- **Python 3.9+**: ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ asyncio ì§€ì› í•„ìˆ˜
- **KoNLPy**: í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ (Mecab, Okt, Komoran ì§€ì›)

### í•„ìˆ˜ ì˜ì¡´ì„±
```python
# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
konlpy>=0.6.0       # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„
aiofiles>=0.8.0     # ë¹„ë™ê¸° íŒŒì¼ I/O
dataclasses         # ë°ì´í„° êµ¬ì¡° ì •ì˜ (Python 3.7+)

# ë‚´ë¶€ ì˜ì¡´ì„±
paca.core.types     # ê³µí†µ íƒ€ì… ì •ì˜
paca.core.events    # ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ
paca.core.utils     # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
```

### ì‹¤í–‰ í™˜ê²½
- **ë©”ëª¨ë¦¬**: ìµœì†Œ 512MB (ë‹¤ì¤‘ ë¶„ì„ê¸° ë¡œë“œì‹œ 1GB ê¶Œì¥)
- **CPU**: ë©€í‹°ì½”ì–´ ê¶Œì¥ (ë³‘ë ¬ ë¶„ì„ ì§€ì›)
- **ìš´ì˜ì²´ì œ**: Windows/Linux/macOS (KoNLPy ì§€ì› í™˜ê²½)

### ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­
- **ì‘ë‹µ ì‹œê°„**: ë¬¸ì¥ë‹¹ <500ms (í˜•íƒœì†Œ ë¶„ì„ ê¸°ì¤€)
- **ì²˜ë¦¬ëŸ‰**: ì´ˆë‹¹ 100ë¬¸ì¥ ì´ìƒ
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ë¶„ì„ê¸°ë‹¹ <100MB ë©”ëª¨ë¦¬ ì‚¬ìš©

## ğŸš€ ë¼ìš°íŒ… ë° ì§„ì…ì 

### ì£¼ìš” ì§„ì…ì  í´ë˜ìŠ¤
```python
# í†µí•© NLP ì‹œìŠ¤í…œ
from paca.integrations.nlp import KoreanNLPSystem

# ê°œë³„ ì»´í¬ë„ŒíŠ¸
from paca.integrations.nlp import (
    KoNLPyIntegration,
    KoreanTokenizer,
    MorphologyAnalyzer,
    SyntaxParser,
    SemanticAnalyzer,
    CulturalContextProcessor
)
```

### API ì§„ì…ì 
```python
# ê°„í¸ ì‚¬ìš© í•¨ìˆ˜ë“¤
from paca.integrations.nlp import get_nlp_system, quick_analyze

# ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸
nlp_system = await get_nlp_system()
result = await nlp_system.process_text("ì•ˆë…•í•˜ì„¸ìš”. ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš”.")

# ê°„ë‹¨ ë¶„ì„
result = await quick_analyze("ë°˜ê°€ì›Œ!", analysis_type="full")
```

### ë¼ìš°íŒ… íŒ¨í„´
- **í†µí•© ë¶„ì„**: `/nlp/analyze` â†’ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
- **í† í°í™”ë§Œ**: `/nlp/tokenize` â†’ í† í°í™”ë§Œ ì‹¤í–‰
- **í˜•íƒœì†Œ ë¶„ì„**: `/nlp/morphology` â†’ í˜•íƒœì†Œ ë¶„ì„ê¹Œì§€
- **ê°ì • ë¶„ì„**: `/nlp/sentiment` â†’ ê°ì • ë¶„ì„ íŠ¹í™”
- **ë¬¸í™” ë¶„ì„**: `/nlp/cultural` â†’ ë¬¸í™”ì  ë§¥ë½ë§Œ

## ğŸ“‹ ì½”ë“œ í’ˆì§ˆ ê°€ì´ë“œ

### ì£¼ì„ ë° ë¬¸ì„œí™” ê·œì¹™
```python
"""
Module: integrations.nlp.{module_name}
Purpose: {ëª¨ë“ˆì˜ êµ¬ì²´ì  ëª©ì  ì„¤ëª…}
Author: PACA Development Team
Created: 2024-09-24
Last Modified: 2024-09-24
"""

class ExampleAnalyzer:
    """
    í•œêµ­ì–´ ë¶„ì„ ì˜ˆì œ í´ë˜ìŠ¤.

    Args:
        config (Dict[str, Any]): ì„¤ì • ë§¤ê°œë³€ìˆ˜

    Example:
        >>> analyzer = ExampleAnalyzer()
        >>> result = await analyzer.analyze("ì•ˆë…•í•˜ì„¸ìš”")
        >>> print(result.sentiment)
    """
```

### ë„¤ì´ë° ì»¨ë²¤ì…˜
- **í´ë˜ìŠ¤**: PascalCase (`KoreanTokenizer`, `MorphologyAnalyzer`)
- **í•¨ìˆ˜/ë©”ì„œë“œ**: snake_case (`analyze_sentiment`, `detect_honorifics`)
- **ìƒìˆ˜**: UPPER_SNAKE_CASE (`HONORIFIC_LEVELS`, `POS_MAPPINGS`)
- **ë³€ìˆ˜**: snake_case (`text_input`, `analysis_result`)

### ì˜ˆì™¸ì²˜ë¦¬ íŒ¨í„´
```python
try:
    result = await analyzer.analyze(text)
    logger.debug(f"Analysis completed: {len(result)} tokens")
    return result
except KoNLPyError as e:
    logger.error(f"KoNLPy analysis failed: {e}")
    # í´ë°± ë¶„ì„ê¸° ì‹œë„
    return await fallback_analyze(text)
except Exception as e:
    logger.error(f"Unexpected error in analysis: {e}")
    raise NLPAnalysisError(f"Analysis failed: {str(e)}")
```

### íƒ€ì… íŒíŒ… ê·œì¹™
```python
from typing import Dict, List, Optional, Any, Union, Tuple
from dataclasses import dataclass
from enum import Enum

async def analyze_text(
    text: str,
    include_sentiment: bool = True,
    config: Optional[Dict[str, Any]] = None
) -> AnalysisResult:
    """ëª¨ë“  ê³µê°œ í•¨ìˆ˜ëŠ” ì™„ì „í•œ íƒ€ì… íŒíŒ… í•„ìˆ˜"""
```

## ğŸƒâ€â™‚ï¸ ì‹¤í–‰ ë°©ë²•

### ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •
```bash
# 1. KoNLPy ì„¤ì¹˜ (Java í•„ìš”)
pip install konlpy

# 2. í˜•íƒœì†Œ ë¶„ì„ê¸° ì„¤ì¹˜ (ì„ íƒì‚¬í•­)
# Mecab ì„¤ì¹˜: https://konlpy.org/en/latest/install/
# ë˜ëŠ” ë‹¤ë¥¸ ë¶„ì„ê¸° ì‚¬ìš© (Okt, Komoran ë“±)

# 3. PACA í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸
cd paca_python
python -c "from paca.integrations.nlp import get_nlp_system; print('NLP ì‹œìŠ¤í…œ ë¡œë“œ ì„±ê³µ')"
```

### ê¸°ë³¸ ì‚¬ìš© ì˜ˆì œ
```python
import asyncio
from paca.integrations.nlp import get_nlp_system

async def main():
    # NLP ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    nlp = await get_nlp_system()

    # ì „ì²´ ë¶„ì„ ì‹¤í–‰
    result = await nlp.process_text(
        "ì•ˆë…•í•˜ì„¸ìš”, êµìˆ˜ë‹˜. ì˜¤ëŠ˜ ìˆ˜ì—… ì˜ ë“¤ì—ˆìŠµë‹ˆë‹¤."
    )

    # ê²°ê³¼ ì¶œë ¥
    print("=== ë¶„ì„ ê²°ê³¼ ===")
    print(f"í† í° ìˆ˜: {len(result['analysis']['tokenization'])}")
    print(f"ì¡´ëŒ“ë§ ìˆ˜ì¤€: {result['analysis']['cultural_context']['honorific_analysis']['overall_level']}")
    print(f"ê°ì • ê·¹ì„±: {result['analysis']['semantics']['sentiment']['polarity']}")

# ì‹¤í–‰
asyncio.run(main())
```

### ê°œë³„ ì»´í¬ë„ŒíŠ¸ ì‚¬ìš©
```python
from paca.integrations.nlp import KoreanTokenizer, SemanticAnalyzer

async def component_example():
    # í† í¬ë‚˜ì´ì € ì‚¬ìš©
    tokenizer = KoreanTokenizer(include_honorifics=True)
    await tokenizer.initialize()

    tokens = await tokenizer.tokenize("ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš”!")
    for token in tokens:
        print(f"{token.text} ({token.token_type.value})")

    # ì˜ë¯¸ ë¶„ì„ê¸° ì‚¬ìš©
    semantic = SemanticAnalyzer()
    await semantic.initialize()

    result = await semantic.analyze("ì •ë§ í–‰ë³µí•œ í•˜ë£¨ì˜€ìŠµë‹ˆë‹¤!")
    print(f"ê°ì •: {result.sentiment.polarity.value}")
    print(f"ê°ì • ê°•ë„: {result.sentiment.intensity:.2f}")
```

### ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•
```python
config = {
    'tokenizer_model': 'mecab',  # or 'okt', 'komoran'
    'morphology': {
        'include_compounds': True,
        'include_phonetics': False
    },
    'semantic': {
        'sentiment_threshold': 0.7,
        'emotion_detection': True
    },
    'cultural': {
        'relationship_inference': True,
        'honorific_suggestions': True
    }
}

nlp = await get_nlp_system(config)
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë°©ë²•

### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
```bash
# ëª¨ë“  NLP ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
python -m pytest paca/integrations/nlp/test_*.py -v

# ê°œë³„ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸
python -m pytest paca/integrations/nlp/test_tokenizer.py -v
python -m pytest paca/integrations/nlp/test_morphology.py -v
python -m pytest paca/integrations/nlp/test_semantic.py -v
```

### í†µí•© í…ŒìŠ¤íŠ¸
```python
# test_nlp_integration.py
import pytest
import asyncio
from paca.integrations.nlp import get_nlp_system

@pytest.mark.asyncio
async def test_full_pipeline():
    nlp = await get_nlp_system()

    test_cases = [
        "ì•ˆë…•í•˜ì„¸ìš”.",                    # ê¸°ë³¸ ì¸ì‚¬
        "êµìˆ˜ë‹˜ê»˜ì„œ ë§ì”€í•˜ì…¨ìŠµë‹ˆë‹¤.",        # ì¡´ëŒ“ë§
        "ë„ˆë¬´ ê¸°ë»ìš”!",                   # ê°ì • í‘œí˜„
        "ìš°ë¦¬ íšŒì‚¬ì—ì„œ ì¼í•©ë‹ˆë‹¤.",          # ë¬¸í™”ì  í‘œí˜„
    ]

    for text in test_cases:
        result = await nlp.process_text(text)
        assert result['status'] == 'success'
        assert 'analysis' in result
        print(f"âœ“ {text} â†’ ë¶„ì„ ì„±ê³µ")

# ì‹¤í–‰
asyncio.run(test_full_pipeline())
```

### ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
```python
import time
import asyncio
from paca.integrations.nlp import get_nlp_system

async def performance_test():
    nlp = await get_nlp_system()

    # 100ê°œ ë¬¸ì¥ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
    test_sentences = ["ì•ˆë…•í•˜ì„¸ìš”. ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”."] * 100

    start_time = time.time()
    for sentence in test_sentences:
        await nlp.process_text(sentence)
    end_time = time.time()

    total_time = end_time - start_time
    avg_time = total_time / len(test_sentences)

    print(f"ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
    print(f"ë¬¸ì¥ë‹¹ í‰ê· : {avg_time*1000:.2f}ms")
    print(f"ì´ˆë‹¹ ì²˜ë¦¬ëŸ‰: {len(test_sentences)/total_time:.1f} ë¬¸ì¥/ì´ˆ")

    # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
    assert avg_time < 0.5  # ë¬¸ì¥ë‹¹ 500ms ë¯¸ë§Œ
    assert len(test_sentences)/total_time > 10  # ì´ˆë‹¹ 10ë¬¸ì¥ ì´ìƒ
```

### í•œêµ­ì–´ íŠ¹í™” í…ŒìŠ¤íŠ¸
```python
async def korean_specific_test():
    nlp = await get_nlp_system()

    # ì¡´ëŒ“ë§ ìˆ˜ì¤€ í…ŒìŠ¤íŠ¸
    formal_text = "ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ? ëµ™ê²Œ ë˜ì–´ ì˜ê´‘ì…ë‹ˆë‹¤."
    casual_text = "ì•ˆë…•? ë°˜ê°€ì›Œ!"

    formal_result = await nlp.process_text(formal_text)
    casual_result = await nlp.process_text(casual_text)

    formal_level = formal_result['analysis']['cultural_context']['honorific_analysis']['overall_level']
    casual_level = casual_result['analysis']['cultural_context']['honorific_analysis']['overall_level']

    assert formal_level in ['elevated', 'polite_formal']
    assert casual_level == 'casual'

    print(f"âœ“ ì¡´ëŒ“ë§ ê°ì§€: {formal_text} â†’ {formal_level}")
    print(f"âœ“ ë°˜ë§ ê°ì§€: {casual_text} â†’ {casual_level}")
```

## ğŸ’¡ ì¶”ê°€ ê³ ë ¤ì‚¬í•­

### ë³´ì•ˆ ê³ ë ¤ì‚¬í•­
- **ê°œì¸ì •ë³´ ì²˜ë¦¬**: í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘ ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹ ê¸°ëŠ¥ ì œê³µ
- **ë¡œê·¸ ë³´ì•ˆ**: ë¯¼ê°í•œ í…ìŠ¤íŠ¸ ë‚´ìš©ì€ ë¡œê·¸ì— ê¸°ë¡í•˜ì§€ ì•ŠìŒ
- **ë©”ëª¨ë¦¬ ê´€ë¦¬**: ë¶„ì„ ì™„ë£Œ í›„ í…ìŠ¤íŠ¸ ë°ì´í„° ì¦‰ì‹œ ì •ë¦¬

### ì„±ëŠ¥ ìµœì í™” ì „ëµ
- **ìºì‹±**: ë™ì¼ í…ìŠ¤íŠ¸ ì¬ë¶„ì„ ë°©ì§€ë¥¼ ìœ„í•œ ê²°ê³¼ ìºì‹±
- **ë°°ì¹˜ ì²˜ë¦¬**: ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ë™ì‹œ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°°ì¹˜ API
- **ì§€ì—° ë¡œë”©**: í•„ìš”í•œ ë¶„ì„ê¸°ë§Œ ì„ íƒì  ë¡œë”©
- **ë©”ëª¨ë¦¬ í’€ë§**: ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ ì¬ì‚¬ìš©ìœ¼ë¡œ ì´ˆê¸°í™” ë¹„ìš© ì ˆì•½

### í–¥í›„ ê°œì„  ê³„íš
1. **ë”¥ëŸ¬ë‹ ëª¨ë¸ í†µí•©**: KoELECTRA, KoBERT ë“± ì‚¬ì „í›ˆë ¨ ëª¨ë¸ ì—°ë™
2. **ë„ë©”ì¸ íŠ¹í™”**: ì˜ë£Œ, ë²•ë¥ , ë‰´ìŠ¤ ë“± ë„ë©”ì¸ë³„ ë¶„ì„ ìµœì í™”
3. **ë‹¤êµ­ì–´ ì§€ì›**: í•œì˜ í˜¼ìš© í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê°œì„ 
4. **ì‹¤ì‹œê°„ ë¶„ì„**: ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì‹¤ì‹œê°„ ì²˜ë¦¬ ì§€ì›
5. **ì‹œê°í™”**: ë¶„ì„ ê²°ê³¼ ì‹œê°í™” ë„êµ¬ ì œê³µ

### ë¬¸í™”ì  ê³ ë ¤ì‚¬í•­
- **ì§€ì—­ë³„ ì°¨ì´**: í‘œì¤€ì–´/ë°©ì–¸ ì°¨ì´ ì¸ì‹ ë° ì²˜ë¦¬
- **ì„¸ëŒ€ë³„ ì–¸ì–´**: MZì„¸ëŒ€, ê¸°ì„±ì„¸ëŒ€ ì–¸ì–´ íŒ¨í„´ ì°¨ì´ ë°˜ì˜
- **ìƒí™©ë³„ ì ì ˆì„±**: ë¹„ì¦ˆë‹ˆìŠ¤/ì¼ìƒ/ê³µì‹ì„ìƒ ë“± ìƒí™©ë³„ ì–¸ì–´ ì ì ˆì„± ê²€ì¦
- **í•œêµ­ ë¬¸í™” ê°œë…**: ëˆˆì¹˜, ì •, í•œ ë“± í•œêµ­ ê³ ìœ  ë¬¸í™” ê°œë… ì¸ì‹

### í™•ì¥ì„± ê³ ë ¤
- **í”ŒëŸ¬ê·¸ì¸ ì•„í‚¤í…ì²˜**: ìƒˆë¡œìš´ ë¶„ì„ê¸° ì‰½ê²Œ ì¶”ê°€ ê°€ëŠ¥
- **API ë²„ì „ ê´€ë¦¬**: í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥í•˜ëŠ” ë²„ì „ ê´€ë¦¬
- **ë¶„ì‚° ì²˜ë¦¬**: ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¶„ì‚° ì•„í‚¤í…ì²˜ ì§€ì›
- **í´ë¼ìš°ë“œ ì—°ë™**: AWS Comprehend, Google Cloud Natural Language ë“± í´ë¼ìš°ë“œ NLP ì„œë¹„ìŠ¤ ì—°ë™

---

## ğŸ“ˆ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ëª©í‘œ

| ë©”íŠ¸ë¦­ | ëª©í‘œê°’ | í˜„ì¬ê°’ | ìƒíƒœ |
|--------|--------|--------|------|
| í˜•íƒœì†Œ ë¶„ì„ ì •í™•ë„ | >95% | êµ¬í˜„ ì™„ë£Œ | âœ… |
| ì¡´ëŒ“ë§ ìˆ˜ì¤€ ê°ì§€ ì •í™•ë„ | >90% | êµ¬í˜„ ì™„ë£Œ | âœ… |
| ê°ì • ë¶„ì„ ì •í™•ë„ | >85% | êµ¬í˜„ ì™„ë£Œ | âœ… |
| ë¬¸ì¥ë‹¹ ì²˜ë¦¬ ì‹œê°„ | <500ms | êµ¬í˜„ ì™„ë£Œ | âœ… |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | <500MB | êµ¬í˜„ ì™„ë£Œ | âœ… |

**Phase 2.2 ì™„ì„±**: í•œêµ­ì–´ NLP í†µí•© ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ êµ¬í˜„ë˜ì–´ PACAì˜ Phase 2 (AI ê¸°ëŠ¥ ê³ ë„í™”)ê°€ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤! ğŸ‰