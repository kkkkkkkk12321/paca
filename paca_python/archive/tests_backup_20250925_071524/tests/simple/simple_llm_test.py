"""
ê°„ë‹¨í•œ LLM í…ŒìŠ¤íŠ¸
API ì„í¬íŠ¸ ë¬¸ì œë¥¼ ìš°íšŒí•˜ì—¬ LLM ê¸°ëŠ¥ë§Œ í…ŒìŠ¤íŠ¸
"""

import asyncio
import os
import sys
from pathlib import Path

# PACA ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent))

# ì§ì ‘ LLM ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from paca.api.llm.base import ModelType, GenerationConfig, LLMRequest
    from paca.api.llm.gemini_client import GeminiClientManager, GeminiConfig
    from paca.api.llm.response_processor import ResponseProcessor
    print("âœ… LLM ëª¨ë“ˆ ì„í¬íŠ¸ ì„±ê³µ")
    LLM_AVAILABLE = True
except Exception as e:
    print(f"âŒ LLM ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    LLM_AVAILABLE = False


async def test_gemini_client():
    """Gemini í´ë¼ì´ì–¸íŠ¸ ì§ì ‘ í…ŒìŠ¤íŠ¸"""
    if not LLM_AVAILABLE:
        print("LLM ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False

    print("\n=== Gemini í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸ ===")

    # API í‚¤ í™•ì¸
    api_keys_env = os.getenv("GEMINI_API_KEYS", "")
    if not api_keys_env:
        print("âš ï¸ GEMINI_API_KEYS í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        print("ğŸ’¡ export GEMINI_API_KEYS=\"your_key1,your_key2\"ë¡œ ì„¤ì •í•˜ì„¸ìš”")

        # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ í‚¤ë¡œ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ë§Œ ì§„í–‰
        api_keys = ["test_key"]
    else:
        api_keys = [key.strip() for key in api_keys_env.split(",")]
        print(f"âœ… {len(api_keys)}ê°œ API í‚¤ ë°œê²¬")

    # í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
    config = GeminiConfig(
        api_keys=api_keys,
        default_model=ModelType.GEMINI_FLASH,
        enable_caching=True,
        timeout=30.0
    )

    # í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    client = GeminiClientManager(config)

    try:
        # ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
        print("\n1. í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”...")
        init_result = await client.initialize()

        if init_result.is_success:
            print("   âœ… ì´ˆê¸°í™” ì„±ê³µ")

            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸ (API í‚¤ê°€ ìˆëŠ” ê²½ìš°)
            if api_keys_env:  # ì‹¤ì œ API í‚¤ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ
                print("\n2. í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸...")

                request = LLMRequest(
                    prompt="ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨íˆ ì¸ì‚¬í•´ì£¼ì„¸ìš”.",
                    model=ModelType.GEMINI_FLASH,
                    config=GenerationConfig(
                        temperature=0.7,
                        max_tokens=100
                    )
                )

                result = await client.generate_text(request)

                if result.is_success:
                    print(f"   âœ… ì‘ë‹µ: {result.data.text[:100]}...")
                    print(f"   â±ï¸ ì²˜ë¦¬ì‹œê°„: {result.data.processing_time:.3f}ì´ˆ")
                else:
                    print(f"   âŒ ì‹¤íŒ¨: {result.error}")
            else:
                print("   âš ï¸ API í‚¤ê°€ ì—†ì–´ í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸ ê±´ë„ˆëœ€")

        else:
            print(f"   âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {init_result.error}")

        # ì •ë¦¬
        await client.cleanup()
        print("\n3. ì •ë¦¬ ì™„ë£Œ")
        return init_result.is_success

    except Exception as e:
        print(f"   âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False


async def test_response_processor():
    """ì‘ë‹µ ì²˜ë¦¬ê¸° í…ŒìŠ¤íŠ¸"""
    if not LLM_AVAILABLE:
        return False

    print("\n=== ì‘ë‹µ ì²˜ë¦¬ê¸° í…ŒìŠ¤íŠ¸ ===")

    try:
        processor = ResponseProcessor()

        # ë”ë¯¸ ì‘ë‹µ ìƒì„±
        from paca.api.llm.base import LLMResponse
        test_response = LLMResponse(
            id="test_123",
            text="ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” PACA AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
            model=ModelType.GEMINI_FLASH,
            usage={"total_tokens": 20},
            processing_time=0.5
        )

        # ì‘ë‹µ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        result = await processor.process_response(
            test_response,
            "ì•ˆë…•í•˜ì„¸ìš”!",
            {}
        )

        if result.is_success:
            print("âœ… ì‘ë‹µ ì²˜ë¦¬ ì„±ê³µ")
            processed_response, metrics = result.data
            print(f"   í’ˆì§ˆ ì ìˆ˜: {metrics.quality_score:.2f}")
            print(f"   í† í° ìˆ˜: {metrics.token_count}")
        else:
            print(f"âŒ ì‘ë‹µ ì²˜ë¦¬ ì‹¤íŒ¨: {result.error}")

        await processor.cleanup()
        return result.is_success if result else False

    except Exception as e:
        print(f"âŒ ì‘ë‹µ ì²˜ë¦¬ê¸° í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {str(e)}")
        return False


async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸"""
    print("PACA v5 LLM ëª¨ë“ˆ ë‹¨ë… í…ŒìŠ¤íŠ¸")
    print("="*40)

    if not LLM_AVAILABLE:
        print("âŒ LLM ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:")
        print("1. pip install google-genai")
        print("2. ëª¨ë“ˆ ê²½ë¡œ ë° import ì˜¤ë¥˜")
        return

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    client_success = await test_gemini_client()
    processor_success = await test_response_processor()

    print("\n" + "="*40)
    print("í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"Gemini í´ë¼ì´ì–¸íŠ¸: {'âœ… ì„±ê³µ' if client_success else 'âŒ ì‹¤íŒ¨'}")
    print(f"ì‘ë‹µ ì²˜ë¦¬ê¸°: {'âœ… ì„±ê³µ' if processor_success else 'âŒ ì‹¤íŒ¨'}")

    if client_success and processor_success:
        print("\nğŸ‰ LLM ëª¨ë“ˆì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤!")
        print("\në‹¤ìŒ ë‹¨ê³„:")
        print("1. GEMINI_API_KEYS í™˜ê²½ë³€ìˆ˜ ì„¤ì •")
        print("2. ì „ì²´ PACA ì‹œìŠ¤í…œê³¼ í†µí•© í…ŒìŠ¤íŠ¸")
    else:
        print("\nâš ï¸ ì¼ë¶€ ê¸°ëŠ¥ì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    asyncio.run(main())