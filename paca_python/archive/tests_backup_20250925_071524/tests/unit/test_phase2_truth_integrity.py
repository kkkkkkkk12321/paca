#!/usr/bin/env python3
"""
Phase 2.2 & 2.3 Integration Test
ì§„ì‹¤ íƒêµ¬ í”„ë¡œí† ì½œ & ì§€ì  ë¬´ê²°ì„± ì ìˆ˜ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸

ìƒˆë¡œ êµ¬í˜„ëœ Phase 2.2 (Truth Seeking Protocol)ì™€ Phase 2.3 (IIS System)ì˜ í†µí•© í…ŒìŠ¤íŠ¸
"""

import asyncio
import sys
import os

# PACA ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'paca'))

from paca.cognitive.truth import TruthSeeker, UncertaintyType
from paca.cognitive.integrity import IntegrityScoring, BehaviorType, IntegrityDimension


class Phase2IntegrationTest:
    """Phase 2.2 & 2.3 í†µí•© í…ŒìŠ¤íŠ¸"""

    def __init__(self):
        print("=" * 80)
        print("PACA Phase 2.2 & 2.3 Integration Test")
        print("ì§„ì‹¤ íƒêµ¬ í”„ë¡œí† ì½œ & ì§€ì  ë¬´ê²°ì„± ì ìˆ˜ ì‹œìŠ¤í…œ")
        print("=" * 80)

    async def test_truth_seeking_system(self):
        """ì§„ì‹¤ íƒêµ¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        print("\n[TRUTH] Testing Truth Seeking System (Phase 2.2)")
        print("-" * 60)

        try:
            # TruthSeeker ì´ˆê¸°í™”
            truth_seeker = TruthSeeker()

            # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
            test_queries = [
                "I'm not sure if this information is accurate",
                "The data shows that climate change is happening, but I need to verify this",
                "This claim seems uncertain and requires fact-checking",
                "Studies show that exercise is good for health"
            ]

            for i, query in enumerate(test_queries, 1):
                print(f"\n{i}. Testing query: {query[:50]}...")

                # ë¶ˆí™•ì‹¤ì„± ê°ì§€
                uncertainty_result = await truth_seeker.detect_uncertainty(query, {})

                if uncertainty_result.is_success:
                    uncertainties = uncertainty_result.data
                    print(f"   [OK] Detected {len(uncertainties)} uncertainty markers")

                    for uncertainty in uncertainties:
                        print(f"      - Type: {uncertainty.uncertainty_type.name}")
                        print(f"      - Confidence: {uncertainty.confidence_level:.2f}")
                        print(f"      - Priority: {uncertainty.priority_score:.2f}")
                else:
                    print(f"   [ERROR] Uncertainty detection failed: {uncertainty_result.error}")
                    continue

                # ì§„ì‹¤ íƒêµ¬ ìˆ˜í–‰
                truth_result = await truth_seeker.seek_truth(query, {'importance': 'high'})

                if truth_result.is_success:
                    result = truth_result.data
                    print(f"   [OK] Truth seeking completed in {result.processing_time:.2f}s")
                    print(f"      - Verification actions: {len(result.verification_actions_taken)}")
                    print(f"      - Knowledge updates: {len(result.knowledge_updates)}")
                    print(f"      - Confidence improvement: {result.confidence_improvement:.2f}")
                    print(f"      - Remaining uncertainties: {len(result.remaining_uncertainties)}")

                    if result.truth_assessment:
                        assessment = result.truth_assessment
                        print(f"      - Truth score: {assessment.truth_score.to_percentage():.1f}%")
                        print(f"      - Confidence level: {assessment.truth_score.confidence_level.name}")
                else:
                    print(f"   [ERROR] Truth seeking failed: {truth_result.error}")

            # í†µê³„ í™•ì¸
            history = truth_seeker.get_seeking_history()
            kb_stats = truth_seeker.get_knowledge_base_stats()

            print(f"\n[STATS] Truth Seeking Statistics:")
            print(f"   - Total seeking operations: {len(history)}")
            print(f"   - Knowledge base entries: {kb_stats['total_entries']}")
            print(f"   - Verified facts: {kb_stats.get('verified_facts', 0)}")
            print(f"   - Uncertain information: {kb_stats.get('uncertain_information', 0)}")

            return True

        except Exception as e:
            print(f"[ERROR] Truth seeking test failed: {str(e)}")
            return False

    async def test_integrity_scoring_system(self):
        """ì§€ì  ë¬´ê²°ì„± ì ìˆ˜ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ¯ Testing Intellectual Integrity Scoring System (Phase 2.3)")
        print("-" * 60)

        try:
            # IntegrityScoring ì´ˆê¸°í™”
            integrity_scoring = IntegrityScoring()

            # ì´ˆê¸° ìƒíƒœ í™•ì¸
            initial_report = integrity_scoring.get_integrity_report()
            print(f"Initial integrity score: {initial_report['overall_metrics']['score']:.1f}")
            print(f"Initial level: {initial_report['overall_metrics']['level']}")

            # ë‹¤ì–‘í•œ í–‰ë™ë“¤ ê¸°ë¡
            test_behaviors = [
                (BehaviorType.TRUTH_SEEKING, {'severity': 'normal'}, "Actively seeking truth"),
                (BehaviorType.SOURCE_CITING, {'severity': 'normal'}, "Citing reliable sources"),
                (BehaviorType.ERROR_CORRECTION, {'severity': 'high'}, "Correcting identified errors"),
                (BehaviorType.UNCERTAINTY_ADMISSION, {'severity': 'normal'}, "Admitting uncertainty"),
                (BehaviorType.BIAS_RECOGNITION, {'severity': 'normal'}, "Recognizing potential bias"),
                (BehaviorType.OVERCONFIDENCE, {'severity': 'low'}, "Showing overconfidence"),
                (BehaviorType.SOURCE_OMISSION, {'severity': 'normal'}, "Omitting source citation")
            ]

            print(f"\nğŸ¬ Recording {len(test_behaviors)} behaviors...")

            for i, (behavior_type, context, description) in enumerate(test_behaviors, 1):
                print(f"\n{i}. Recording: {behavior_type.name}")
                print(f"   Description: {description}")

                result = await integrity_scoring.record_behavior(
                    behavior_type=behavior_type,
                    context=context,
                    evidence=[description],
                    confidence=0.85
                )

                if result.is_success:
                    action = result.data
                    print(f"   âœ… Recorded with score impact: {action.score_impact:.2f}")
                    print(f"      - Primary dimension: {action.dimension.name}")
                    print(f"      - Confidence: {action.confidence:.2f}")
                else:
                    print(f"   âŒ Failed to record: {result.error}")

            # ìµœì¢… ë³´ê³ ì„œ ìƒì„±
            final_report = integrity_scoring.get_integrity_report()

            print(f"\nğŸ“Š Final Integrity Report:")
            print(f"   - Overall Score: {final_report['overall_metrics']['score']:.1f}")
            print(f"   - Integrity Level: {final_report['overall_metrics']['level']}")
            print(f"   - Trend: {final_report['overall_metrics']['trend']}")
            print(f"   - Reliability: {final_report['overall_metrics']['reliability']:.2f}")
            print(f"   - Consistency: {final_report['overall_metrics']['consistency']:.2f}")

            print(f"\nğŸ“ˆ Dimension Scores:")
            for dimension, score in final_report['dimension_scores'].items():
                print(f"   - {dimension}: {score:.1f}")

            print(f"\nğŸ“‹ Recent Activity:")
            activity = final_report['recent_activity']
            print(f"   - Total actions: {activity['total_actions']}")
            print(f"   - Positive actions: {activity['positive_actions']}")
            print(f"   - Negative actions: {activity['negative_actions']}")

            # ê±°ì§“ë§ íƒì§€ í…ŒìŠ¤íŠ¸
            print(f"\nğŸ•µï¸ Testing Dishonesty Detection:")

            test_content = """
            I am absolutely certain that this claim is 100% true.
            Studies show clear evidence, and research proves this beyond doubt.
            There is no uncertainty whatsoever in this statement.
            """

            dishonesty_result = await integrity_scoring.detect_dishonesty(
                test_content, {'source': 'test'}
            )

            if dishonesty_result.is_success:
                indicators = dishonesty_result.data
                print(f"   âœ… Detected {len(indicators)} dishonesty indicators:")
                for indicator in indicators:
                    print(f"      - {indicator}")
            else:
                print(f"   âŒ Dishonesty detection failed: {dishonesty_result.error}")

            # ì‹ ë¢°ë„ ì ìˆ˜ í™•ì¸
            trust_score = await integrity_scoring.get_trust_score()
            print(f"\nğŸ¤ Trust Score: {trust_score:.3f}")

            return True

        except Exception as e:
            print(f"âŒ Integrity scoring test failed: {str(e)}")
            return False

    async def test_integration_workflow(self):
        """í†µí•© ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        print("\nğŸ”„ Testing Integrated Workflow (Truth Seeking + Integrity Scoring)")
        print("-" * 60)

        try:
            # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            truth_seeker = TruthSeeker()
            integrity_scoring = IntegrityScoring()

            # ì‹œë‚˜ë¦¬ì˜¤: ë¶ˆí™•ì‹¤í•œ ì •ë³´ì— ëŒ€í•œ ì§„ì‹¤ íƒêµ¬ì™€ ë¬´ê²°ì„± í‰ê°€
            scenario_query = "I believe this claim is true, but I'm not completely certain about it"

            print(f"Scenario: {scenario_query}")

            # 1. ë¶ˆí™•ì‹¤ì„± ê°ì§€
            print(f"\n1ï¸âƒ£ Detecting uncertainty...")
            uncertainty_result = await truth_seeker.detect_uncertainty(scenario_query, {})

            if uncertainty_result.is_success and uncertainty_result.data:
                print(f"   âœ… Found uncertainties - triggering truth seeking behavior")

                # ë¬´ê²°ì„± ì ìˆ˜: ì§„ì‹¤ íƒêµ¬ í–‰ë™ ê¸°ë¡
                await integrity_scoring.record_behavior(
                    BehaviorType.TRUTH_SEEKING,
                    {'trigger': 'uncertainty_detected'},
                    [f"Uncertainty detected in: {scenario_query[:50]}..."]
                )

            # 2. ì§„ì‹¤ íƒêµ¬ ìˆ˜í–‰
            print(f"\n2ï¸âƒ£ Performing truth seeking...")
            truth_result = await truth_seeker.seek_truth(scenario_query)

            if truth_result.is_success:
                result = truth_result.data
                print(f"   âœ… Truth seeking completed")

                # ë¬´ê²°ì„± ì ìˆ˜: ê²€ì¦ í–‰ë™ ê¸°ë¡
                await integrity_scoring.record_behavior(
                    BehaviorType.VERIFICATION,
                    {'verification_actions': len(result.verification_actions_taken)},
                    result.verification_actions_taken
                )

                # 3. ê²°ê³¼ì— ë”°ë¥¸ ë¬´ê²°ì„± í–‰ë™ ê¸°ë¡
                if result.truth_assessment:
                    assessment = result.truth_assessment
                    truth_score = assessment.truth_score.overall_score

                    if truth_score >= 0.8:
                        # ë†’ì€ ì‹ ë¢°ë„ - ì§€ì‹ ì—…ë°ì´íŠ¸
                        await integrity_scoring.record_behavior(
                            BehaviorType.KNOWLEDGE_UPDATE,
                            {'truth_score': truth_score},
                            [f"High-confidence information verified: {truth_score:.2f}"]
                        )
                    elif truth_score < 0.5:
                        # ë‚®ì€ ì‹ ë¢°ë„ - ë¶ˆí™•ì‹¤ì„± ì¸ì •
                        await integrity_scoring.record_behavior(
                            BehaviorType.UNCERTAINTY_ADMISSION,
                            {'truth_score': truth_score},
                            [f"Low-confidence information acknowledged: {truth_score:.2f}"]
                        )

            # 4. ìµœì¢… í†µí•© ë³´ê³ ì„œ
            print(f"\nğŸ“‹ Integrated Workflow Results:")

            # ì§„ì‹¤ íƒêµ¬ ê²°ê³¼
            seeking_stats = truth_seeker.get_knowledge_base_stats()
            print(f"   Truth Seeking:")
            print(f"   - Knowledge base entries: {seeking_stats['total_entries']}")
            print(f"   - Average confidence: {seeking_stats.get('average_confidence', 0):.2f}")

            # ë¬´ê²°ì„± ì ìˆ˜ ê²°ê³¼
            integrity_report = integrity_scoring.get_integrity_report()
            print(f"   Integrity Scoring:")
            print(f"   - Overall score: {integrity_report['overall_metrics']['score']:.1f}")
            print(f"   - Recent positive actions: {integrity_report['recent_activity']['positive_actions']}")

            return True

        except Exception as e:
            print(f"âŒ Integration workflow test failed: {str(e)}")
            return False

    async def run_all_tests(self):
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("Starting comprehensive Phase 2.2 & 2.3 integration tests...\n")

        results = []

        # Truth Seeking System í…ŒìŠ¤íŠ¸
        truth_seeking_result = await self.test_truth_seeking_system()
        results.append(("Truth Seeking System", truth_seeking_result))

        # Integrity Scoring System í…ŒìŠ¤íŠ¸
        integrity_scoring_result = await self.test_integrity_scoring_system()
        results.append(("Integrity Scoring System", integrity_scoring_result))

        # í†µí•© ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
        integration_result = await self.test_integration_workflow()
        results.append(("Integration Workflow", integration_result))

        # ê²°ê³¼ ìš”ì•½
        print(f"\n" + "=" * 80)
        print("TEST RESULTS SUMMARY")
        print("=" * 80)

        passed = 0
        total = len(results)

        for test_name, result in results:
            status = "âœ… PASSED" if result else "âŒ FAILED"
            print(f"{test_name:.<50} {status}")
            if result:
                passed += 1

        print(f"\nOverall: {passed}/{total} tests passed ({passed/total*100:.1f}%)")

        if passed == total:
            print("ğŸ‰ All Phase 2.2 & 2.3 integration tests PASSED!")
            print("âœ… Truth Seeking Protocol and IIS System are working correctly")
        else:
            print("âš ï¸  Some tests failed. Please check the error messages above.")

        return passed == total


async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    test_runner = Phase2IntegrationTest()
    success = await test_runner.run_all_tests()

    return 0 if success else 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)